{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_x, data_y):\n",
    "        self.data_x = torch.tensor(data_x, dtype=torch.float32)\n",
    "        self.data_y = torch.tensor(data_y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_x[idx], self.data_y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=9, out_channels=32, kernel_size=11,padding=5)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=11,padding=5)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.pool = nn.MaxPool1d(3)\n",
    "        self.lstm1 = nn.LSTM(input_size=32, hidden_size=64, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=64, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.permute(0, 2, 1)  \n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"亂數種子\"\n",
    "np.random.seed(20)\n",
    "\n",
    "def mkdir(path):\n",
    "    #判斷目錄是否存在，存在：True、不存在：False\n",
    "    folder = os.path.exists(path)\n",
    "    if not folder:\n",
    "        os.makedirs(path)\n",
    "\n",
    "filename = []\n",
    "#bad_dataset = [1,2,10,30,49,52,66,83,97,101,114,117,121] #<=137是清福的長輩、<=67是65歲以上長輩\n",
    "bad_dataset = [1,2,10,30,49,52,66,83,94,97,101,107,114,117,121,124] #<=137是清福的長輩、<=67是65歲以上長輩、扣掉離群資料\n",
    "y_data = pd.read_csv('ScaleScore/BBS.csv')\n",
    "\n",
    "\"取csv檔並排除有問題的受測者\"\n",
    "DatasetPath = \"BalanceDataSet_150/\"\n",
    "\n",
    "filepath = os.listdir(DatasetPath)\n",
    "for files in filepath:\n",
    "    if files.endswith(\".csv\"):  #只留副檔名為.csv檔\n",
    "        #if int(files[0:3]) < 137 and int(files[0:3]) > 65 and not (int(files[0:3]) in bad_dataset):    #只使用長輩資料(61位)\n",
    "        if int(files[0:3]) < 137 and not (int(files[0:3]) in bad_dataset):  #只使用年輕人與舊庄長輩資料(120位)\n",
    "            filename.append(files)\n",
    "\n",
    "\"從IMU 1 跑到 IMU 7\"\n",
    "for sensor in range(1,8):       #設定要讀哪一顆IMU的資料(1~7)\n",
    "    \n",
    "    subject_gait_data_x = []      #存每個受測者以切完的步態資料\n",
    "    subject_gait_data_y = []      #存每個受測者的分數\n",
    "    \n",
    "    for files in filename:\n",
    "        gait_x = []\n",
    "        gait_y = []\n",
    "        data = pd.read_csv(DatasetPath + files)\n",
    "        print(DatasetPath + files)\n",
    "               \n",
    "        \"刪除夾角資料\"\n",
    "        data_col_name2 = data.columns \n",
    "        for col_name2 in data_col_name2:\n",
    "            if col_name2[0:5] == \"Right\" or col_name2[0:4] == \"Left\":\n",
    "                data = data.drop(col_name2, axis = 1)\n",
    "          \n",
    "        \"刪除其他sensor資料\"   \n",
    "        sensor_id_A = \"BID\" + str(sensor)    \n",
    "        data_col_name3 = data.columns \n",
    "        for col_name3 in data_col_name3:            \n",
    "            if col_name3[0:3] == \"BID\" and col_name3[0:4] != sensor_id_A:  #指定要留的sensor資料，其他刪除(單顆)\n",
    "                data = data.drop(col_name3, axis = 1)\n",
    "        \n",
    "        row , colume = data.shape        \n",
    "        \n",
    "        \"取得量表分數\"\n",
    "        scale_score = y_data.loc[y_data['ID'] == int(files[0:3]) ,'score'].values\n",
    "        print(scale_score)\n",
    "        \n",
    "        \"找出每個資料的頭跟尾\"\n",
    "        task_ID = 'N'\n",
    "        number = 1\n",
    "        task_StartEnd = pd.DataFrame(columns=[\"task\",\"start\",\"end\"])\n",
    "        f=0\n",
    "        i=0\n",
    "        while i<row: \n",
    "            if data.iloc[i,1] == task_ID and f == 0:\n",
    "                f=1\n",
    "                task_StartEnd.loc[number , \"task\"] = task_ID\n",
    "                task_StartEnd.loc[number , \"start\"] = i          \n",
    "            elif f == 1 and data.iloc[i,1] !=task_ID:\n",
    "                f=0\n",
    "                task_StartEnd.loc[number , \"end\"] = i-1\n",
    "                number += 1\n",
    "                break\n",
    "            i+=1\n",
    "        if f == 1:\n",
    "            task_StartEnd.loc[number , \"end\"] = i-1  \n",
    "        \n",
    "        #print(task_StartEnd)\n",
    "\n",
    "        \"設定window size、前後筆資料重複率\"\n",
    "        window_size = 150       #sample rate = 50Hz，取一秒的資料\n",
    "        Repeat_ratio = int(window_size * ((100-50)/100)) \n",
    "        \n",
    "        \"切割訓練資料\"\n",
    "        task_ID = 'N'\n",
    "        \n",
    "        start = task_StartEnd.at[1 , 'start']\n",
    "        end = task_StartEnd.at[1 , 'end'] + 1\n",
    "        \n",
    "        \n",
    "        total = end - start\n",
    "        training_set = data.iloc[start:end,5:colume].values #取7顆IMU的9軸資料，並且依照動作T、U、V、W、X、Y動作分段取\n",
    "        \n",
    "        \"Z-score Standardization\"\n",
    "        training_set = stats.zscore(training_set)\n",
    "\n",
    "        if scale_score !=56 :\n",
    "            training_set = np.tile(training_set,(4,1))\n",
    "        \n",
    "        \n",
    "        for j in range(0 , training_set.shape[0] , Repeat_ratio):\n",
    "            if j + window_size <= training_set.shape[0]:\n",
    "                gait_x.append(training_set[j:j + window_size])\n",
    "                gait_y.append(scale_score)                   #切好的資料lable\n",
    "\n",
    "        gait_x = np.array(gait_x)\n",
    "        gait_y = np.array(gait_y)      \n",
    "        subject_gait_data_x.append(gait_x)\n",
    "        subject_gait_data_y.append(scale_score)\n",
    "       \n",
    "    \n",
    "    \"打亂受測者的順序\"\n",
    "    r = list(zip(subject_gait_data_x,subject_gait_data_y))\n",
    "    np.random.shuffle(r)\n",
    "    subject_gait_data_x,subject_gait_data_y = zip(*r)\n",
    "    \n",
    "    \"量表分數 normalization\"\n",
    "    subject_gait_data_y = np.array(subject_gait_data_y)     #從別的資料型態轉成array\n",
    "    BBS = [[0],[56]]                                        #BBS的分數範圍\n",
    "    BBS_array = np.array(BBS)\n",
    "    scaler = MinMaxScaler(feature_range=(0,1)).fit(BBS_array)\n",
    "    subject_gait_data_y = scaler.transform(subject_gait_data_y)\n",
    "\n",
    "    \"CNN-LSTM model\"\n",
    "    import time\n",
    "    savepath = 'save/'\n",
    "    if not os.path.isdir(savepath):\n",
    "        os.mkdir(savepath)\n",
    "    savepath = savepath + 'S2S(TaskN)/'\n",
    "    if not os.path.isdir(savepath):\n",
    "        os.mkdir(savepath)\n",
    "    \n",
    "    n=0\n",
    "    epoch_range = [500]                             #設定epoch實驗參數\n",
    "    batch_size_range = [64]                       #設定batch size實驗參數\n",
    "    for epochs in epoch_range:                    #實驗epoch\n",
    "        for batch_size in batch_size_range:       #實驗batch size\n",
    "            n=n+1    \n",
    "            localtime = time.localtime(time.time())\n",
    "            savepath = 'save/S2S(TaskN)/' +str(localtime[0])+str(localtime[1]).zfill(2)+str(localtime[2]).zfill(2)+'v'+str(n)+'_sensor'+str(sensor)+'/'\n",
    "            if not os.path.isdir(savepath):\n",
    "                os.mkdir(savepath)        \n",
    "             \n",
    "            val_times = 5       # k fold cross validation\n",
    "            MAE_average = 0\n",
    "            times = 0\n",
    "            \n",
    "            subject_size = len(subject_gait_data_x)\n",
    "            for i in range(val_times):\n",
    "                times += 1\n",
    "                x_test_subject = subject_gait_data_x[math.floor(i/val_times*subject_size) : math.floor((i+1)/val_times*subject_size)]\n",
    "                y_test_subject = subject_gait_data_y[math.floor(i/val_times*subject_size) : math.floor((i+1)/val_times*subject_size)]\n",
    "                \n",
    "                if i == 0:\n",
    "                    x_train_subject = subject_gait_data_x[math.floor((i+1)/val_times*subject_size) : subject_size]\n",
    "                    y_train_subject = subject_gait_data_y[math.floor((i+1)/val_times*subject_size) : subject_size]\n",
    "                elif i == val_times-1:\n",
    "                    x_train_subject = subject_gait_data_x[0 : math.floor(i/val_times*subject_size)]\n",
    "                    y_train_subject = subject_gait_data_y[0 : math.floor(i/val_times*subject_size)]\n",
    "                else:\n",
    "                    x_train_subject_1 = subject_gait_data_x[0 : math.floor(i/val_times*subject_size)]\n",
    "                    x_train_subject_2 = subject_gait_data_x[math.floor((i+1)/val_times*subject_size) : subject_size]\n",
    "                    x_train_subject = x_train_subject_1 + x_train_subject_2\n",
    "    \n",
    "                    y_train_subject_1 = subject_gait_data_y[0 : math.floor(i/val_times*subject_size)]\n",
    "                    y_train_subject_2 = subject_gait_data_y[math.floor((i+1)/val_times*subject_size) : subject_size]\n",
    "                    y_train_subject = np.concatenate([y_train_subject_1 , y_train_subject_2])                \n",
    "    \n",
    "                x_train = []\n",
    "                y_train = []\n",
    "                for j in range(len(x_train_subject)):\n",
    "                    for k in range(len(x_train_subject[j])):    \n",
    "                        x_train.append(x_train_subject[j][k])\n",
    "                        y_train.append(y_train_subject[j])               \n",
    "                x_train = np.array(x_train)\n",
    "                y_train = np.array(y_train)\n",
    "                \n",
    "                x_test = []\n",
    "                y_test = []\n",
    "                for j in range(len(x_test_subject)):\n",
    "                    for k in range(len(x_test_subject[j])):    \n",
    "                        x_test.append(x_test_subject[j][k])\n",
    "                        y_test.append(y_test_subject[j])\n",
    "                x_test = np.array(x_test)\n",
    "                y_test = np.array(y_test)\n",
    "                \n",
    "                \"打亂訓練資料\"\n",
    "                index = np.random.permutation(x_train.shape[0])\n",
    "                x_train = x_train[index]\n",
    "                y_train = y_train[index]\n",
    "                \n",
    "                \"打亂測試資料\"\n",
    "                index = np.random.permutation(x_test.shape[0])\n",
    "                x_test = x_test[index]\n",
    "                y_test = y_test[index]\n",
    "            \n",
    "                print(x_train.shape , x_test.shape , y_train.shape , y_test.shape)\n",
    "                train_dataset = MyDataset(x_train, y_train)\n",
    "                test_dataset = MyDataset(x_test, y_test)\n",
    "\n",
    "                train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "                test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "                # 訓練部分\n",
    "                model = CNN_LSTM().to(DEVICE)\n",
    "                criterion = nn.MSELoss()\n",
    "                criterion_mae = nn.L1Loss()\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "                for epoch in range(epochs):\n",
    "                    model.train()\n",
    "                    for i,data in enumerate(train_loader):\n",
    "                        x_batch, y_batch = data\n",
    "                        input=x_batch.to(DEVICE)\n",
    "                        label=y_batch.to(DEVICE)\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(input)\n",
    "                        loss = criterion(outputs, label)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    test_loss = 0.0\n",
    "                    steps=0\n",
    "                    for i,data in enumerate(test_loader):\n",
    "                        x_batch, y_batch = data\n",
    "                        input=x_batch.to(DEVICE)\n",
    "                        label=y_batch.to(DEVICE)\n",
    "                        outputs = model(input)\n",
    "                        \"將量表分數轉換成原來分數\"\n",
    "                        y_test_tran = label.cpu().detach().numpy()\n",
    "                        y_pred_tran = outputs.cpu().detach().numpy()\n",
    "\n",
    "                        y_test_tran = scaler.inverse_transform(y_test_tran)\n",
    "                        y_pred_tran = scaler.inverse_transform(y_pred_tran)\n",
    "                        y_pred_tran =np.around(y_pred_tran)\n",
    "\n",
    "                        y_test_tran_tensor = torch.tensor(y_test_tran, dtype=torch.float32).to(DEVICE)\n",
    "                        y_pred_tran_tensor = torch.tensor(y_pred_tran, dtype=torch.float32).to(DEVICE)\n",
    "                        loss = criterion_mae(y_pred_tran_tensor, y_test_tran_tensor)\n",
    "                        test_loss += loss.item()\n",
    "                        steps+=1\n",
    "                    test_loss /= len(test_loader)\n",
    "                    print(f'Test Loss: {test_loss:.4f}')\n",
    "                    MAE_average = MAE_average + test_loss\n",
    "            print(\"BBS MAE:\")\n",
    "            print(MAE_average/val_times)\n",
    "\n",
    "            f = open(savepath + '實驗結果.txt','a')\n",
    "            f.write('window size = '+ str(window_size)+'\\n')\n",
    "            f.write('epoch = ' + str(epochs) + '\\n')\n",
    "            f.write('batch size = ' + str(batch_size) + '\\n')\n",
    "            f.write('MAE = ' + str(MAE_average/val_times) + '\\n')\n",
    "            f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "9BuyWwE_XdRx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import scipy.stats as stats\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Lpc6slfkXdR0"
      },
      "outputs": [],
      "source": [
        "\"亂數種子\"\n",
        "np.random.seed(20)\n",
        "\n",
        "def mkdir(path):\n",
        "    #判斷目錄是否存在，存在：True、不存在：False\n",
        "    folder = os.path.exists(path)\n",
        "    if not folder:\n",
        "        os.makedirs(path)\n",
        "\n",
        "filename = []\n",
        "#bad_dataset = [1,2,10,30,49,52,66,83,97,101,114,117,121] #<=137是清福的長輩、<=67是65歲以上長輩\n",
        "bad_dataset = [1,2,10,30,49,52,66,83,94,97,101,107,114,117,121,124] #<=137是清福的長輩、<=67是65歲以上長輩、扣掉離群資料\n",
        "y_data = pd.read_csv('ScaleScore/BBS.csv')\n",
        "\n",
        "\"取csv檔並排除有問題的受測者\"\n",
        "DatasetPath = \"BalanceDataSet_150/\"\n",
        "\n",
        "filepath = os.listdir(DatasetPath)\n",
        "for files in filepath:\n",
        "    if files.endswith(\".csv\"):  #只留副檔名為.csv檔\n",
        "        #if int(files[0:3]) < 137 and int(files[0:3]) > 65 and not (int(files[0:3]) in bad_dataset):    #只使用長輩資料(61位)\n",
        "        if int(files[0:3]) < 137 and not (int(files[0:3]) in bad_dataset):  #只使用年輕人與舊庄長輩資料(120位)\n",
        "            filename.append(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXiGSZcVXdR1",
        "outputId": "cb57f172-4450-4749-bea3-a96476105fa3"
      },
      "outputs": [],
      "source": [
        "DEVICE=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "G5aHAiPOXdR2"
      },
      "outputs": [],
      "source": [
        "def split_and_shuffle(subject_gait_data_x, subject_gait_data_y,val_times,subject_size,i):\n",
        "    x_test_subject = subject_gait_data_x[math.floor(i/val_times*subject_size) : math.floor((i+1)/val_times*subject_size)]\n",
        "    y_test_subject = subject_gait_data_y[math.floor(i/val_times*subject_size) : math.floor((i+1)/val_times*subject_size)]\n",
        "\n",
        "    if i == 0:\n",
        "        x_train_subject = subject_gait_data_x[math.floor((i+1)/val_times*subject_size) : subject_size]\n",
        "        y_train_subject = subject_gait_data_y[math.floor((i+1)/val_times*subject_size) : subject_size]\n",
        "    elif i == val_times-1:\n",
        "        x_train_subject = subject_gait_data_x[0 : math.floor(i/val_times*subject_size)]\n",
        "        y_train_subject = subject_gait_data_y[0 : math.floor(i/val_times*subject_size)]\n",
        "    else:\n",
        "        x_train_subject_1 = subject_gait_data_x[0 : math.floor(i/val_times*subject_size)]\n",
        "        x_train_subject_2 = subject_gait_data_x[math.floor((i+1)/val_times*subject_size) : subject_size]\n",
        "        x_train_subject = x_train_subject_1 + x_train_subject_2\n",
        "\n",
        "        y_train_subject_1 = subject_gait_data_y[0 : math.floor(i/val_times*subject_size)]\n",
        "        y_train_subject_2 = subject_gait_data_y[math.floor((i+1)/val_times*subject_size) : subject_size]\n",
        "        y_train_subject = np.concatenate([y_train_subject_1 , y_train_subject_2])\n",
        "\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    for j in range(len(x_train_subject)):\n",
        "        for k in range(len(x_train_subject[j])):\n",
        "            x_train.append(x_train_subject[j][k])\n",
        "            y_train.append(y_train_subject[j])\n",
        "    x_train = np.array(x_train)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    x_test = []\n",
        "    y_test = []\n",
        "    for j in range(len(x_test_subject)):\n",
        "        for k in range(len(x_test_subject[j])):\n",
        "            x_test.append(x_test_subject[j][k])\n",
        "            y_test.append(y_test_subject[j])\n",
        "    x_test = np.array(x_test)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "    \"打亂訓練資料\"\n",
        "    index = np.random.permutation(x_train.shape[0])\n",
        "    x_train = x_train[index]\n",
        "    y_train = y_train[index]\n",
        "\n",
        "    \"打亂測試資料\"\n",
        "    index = np.random.permutation(x_test.shape[0])\n",
        "    x_test = x_test[index]\n",
        "    y_test = y_test[index]\n",
        "\n",
        "    return x_train, y_train, x_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "0I-0POksXdR2"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(nn.Module): \n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.con1=nn.Conv1d(in_channels=9, out_channels=32, kernel_size=11,padding=5)\n",
        "        self.fc1=nn.BatchNorm1d(32)\n",
        "        self.fc2=nn.ReLU()\n",
        "        self.con2=nn.Conv1d(in_channels=32, out_channels=32, kernel_size=11,padding=5)\n",
        "        self.fc3=nn.BatchNorm1d(32)\n",
        "        self.fc4=nn.ReLU()\n",
        "        self.fc5=nn.MaxPool1d(3)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.con1(x)\n",
        "        x=self.fc1(x)\n",
        "        x=self.fc2(x)\n",
        "        x=self.con2(x)\n",
        "        x=self.fc3(x)\n",
        "        x=self.fc4(x)\n",
        "        x=self.fc5(x)\n",
        "\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "bhsIo0xpXdR2"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module): \n",
        "    def __init__(self):\n",
        "       super(Classifier, self).__init__()\n",
        "       self.lstm1 = nn.LSTM(32, 64, batch_first=True)\n",
        "       self.dropout1 = nn.Dropout(0.5)\n",
        "       self.lstm2 = nn.LSTM(64, 64, batch_first=True)\n",
        "       self.dropout2 = nn.Dropout(0.2)\n",
        "       self.fc = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x, _ = self.lstm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        last_output = x[:, -1, :]\n",
        "        last_output = self.fc(last_output)\n",
        "\n",
        "        return(last_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ND5sQM6eXdR3"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data_x, data_y):\n",
        "        self.data_x = torch.tensor(data_x, dtype=torch.float32)\n",
        "        self.data_y = torch.tensor(data_y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data_x[idx], self.data_y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2emtrRHXdR3",
        "outputId": "96a1c81a-6737-40eb-b863-d6548ddab750"
      },
      "outputs": [],
      "source": [
        "sensor_idd=[1,2,3,4,5,6,7]\n",
        "for sensor in sensor_idd:\n",
        "    print(sensor)\n",
        "    sensor_id_A = \"BID\" + str(sensor)\n",
        "    sensor_id=['BID1','BID2','BID3','BID4','BID5','BID6','BID7']\n",
        "    sensor_id_src = [sensor_id_A]\n",
        "    if sensor_id_A in sensor_id:\n",
        "        sensor_id.remove(sensor_id_A)\n",
        "    sensor_id_trg = sensor_id\n",
        "\n",
        "    subject_gait_data_x_src = []      #存每個受測者以切完的步態資料\n",
        "    subject_gait_data_y_src = []      #存每個受測者的分數\n",
        "    subject_gait_data_x_trg1 = []      #存每個受測者以切完的步態資料\n",
        "    subject_gait_data_y_trg1 = []      #存每個受測者的分數\n",
        "    subject_gait_data_x_trg2 = []      #存每個受測者以切完的步態資料\n",
        "    subject_gait_data_y_trg2 = []      #存每個受測者的分數\n",
        "    subject_gait_data_x_trg3 = []      #存每個受測者以切完的步態資料\n",
        "    subject_gait_data_y_trg3 = []      #存每個受測者的分數\n",
        "    subject_gait_data_x_trg4 = []      #存每個受測者以切完的步態資料\n",
        "    subject_gait_data_y_trg4 = []      #存每個受測者的分數\n",
        "    subject_gait_data_x_trg5 = []      #存每個受測者以切完的步態資料\n",
        "    subject_gait_data_y_trg5 = []      #存每個受測者的分數\n",
        "    subject_gait_data_x_trg6 = []      #存每個受測者以切完的步態資料\n",
        "    subject_gait_data_y_trg6 = []      #存每個受測者的分數\n",
        "\n",
        "    cols_to_drop_src = []\n",
        "    cols_to_drop_trg1 = []\n",
        "    cols_to_drop_trg2 = []\n",
        "    cols_to_drop_trg3 = []\n",
        "    cols_to_drop_trg4 = []\n",
        "    cols_to_drop_trg5 = []\n",
        "    cols_to_drop_trg6 = []\n",
        "\n",
        "    for files in filename:\n",
        "        gait_x_src = []\n",
        "        gait_y_src = []\n",
        "        gait_x_trg1 = []\n",
        "        gait_y_trg1 = []\n",
        "        gait_x_trg2 = []\n",
        "        gait_y_trg2 = []\n",
        "        gait_x_trg3 = []\n",
        "        gait_y_trg3 = []\n",
        "        gait_x_trg4 = []\n",
        "        gait_y_trg4 = []\n",
        "        gait_x_trg5 = []\n",
        "        gait_y_trg5 = []\n",
        "        gait_x_trg6 = []\n",
        "        gait_y_trg6 = []\n",
        "        data = pd.read_csv(DatasetPath + files)\n",
        "        print(DatasetPath + files)\n",
        "\n",
        "\n",
        "        \"刪除夾角資料\"\n",
        "        data_col_name2 = data.columns\n",
        "        for col_name2 in data_col_name2:\n",
        "            if col_name2[0:5] == \"Right\" or col_name2[0:4] == \"Left\":\n",
        "                data = data.drop(col_name2, axis = 1)\n",
        "\n",
        "        \"刪除其他sensor資料\"\n",
        "        data_col_name3 = data.columns\n",
        "        for col_name3 in data_col_name3:\n",
        "            if col_name3[0:3] == \"BID\" :\n",
        "                if (col_name3[0:4] != sensor_id_A):  #指定要留的sensor資料，其他刪除(單顆)\n",
        "                    cols_to_drop_src.append(col_name3)\n",
        "                if (col_name3[0:4] != sensor_id_trg[0]):  #指定要留的sensor資料，其他刪除(單顆)\n",
        "                    cols_to_drop_trg1.append(col_name3)\n",
        "                if (col_name3[0:4] != sensor_id_trg[1]):  #指定要留的sensor資料，其他刪除(單顆)\n",
        "                    cols_to_drop_trg2.append(col_name3)\n",
        "                if (col_name3[0:4] != sensor_id_trg[2]):  #指定要留的sensor資料，其他刪除(單顆)\n",
        "                    cols_to_drop_trg3.append(col_name3)\n",
        "                if (col_name3[0:4] != sensor_id_trg[3]):  #指定要留的sensor資料，其他刪除(單顆)\n",
        "                    cols_to_drop_trg4.append(col_name3)\n",
        "                if (col_name3[0:4] != sensor_id_trg[4]):  #指定要留的sensor資料，其他刪除(單顆)\n",
        "                    cols_to_drop_trg5.append(col_name3)\n",
        "                if (col_name3[0:4] != sensor_id_trg[5]):  #指定要留的sensor資料，其他刪除(單顆)\n",
        "                    cols_to_drop_trg6.append(col_name3)\n",
        "\n",
        "        data_src = data.drop(columns=cols_to_drop_src)\n",
        "        data_trg1 = data.drop(columns=cols_to_drop_trg1)\n",
        "        data_trg2 = data.drop(columns=cols_to_drop_trg2)\n",
        "        data_trg3 = data.drop(columns=cols_to_drop_trg3)\n",
        "        data_trg4 = data.drop(columns=cols_to_drop_trg4)\n",
        "        data_trg5 = data.drop(columns=cols_to_drop_trg5)\n",
        "        data_trg6 = data.drop(columns=cols_to_drop_trg6)\n",
        "\n",
        "        row , colume = data.shape\n",
        "\n",
        "        \"取得量表分數\"\n",
        "        scale_score = y_data.loc[y_data['ID'] == int(files[0:3]) ,'score'].values\n",
        "        print(scale_score)\n",
        "\n",
        "        \"找出每個資料的頭跟尾\"\n",
        "        task_ID = 'T'\n",
        "        number = 1\n",
        "        task_StartEnd = pd.DataFrame(columns=[\"task\",\"start\",\"end\"])\n",
        "        f=0\n",
        "        i=0\n",
        "        while i<row:\n",
        "            if data.iloc[i,1] == task_ID and f == 0:\n",
        "                f=1\n",
        "                task_StartEnd.loc[number , \"task\"] = task_ID\n",
        "                task_StartEnd.loc[number , \"start\"] = i\n",
        "            elif f == 1 and data.iloc[i,1] !=task_ID:\n",
        "                f=0\n",
        "                task_StartEnd.loc[number , \"end\"] = i-1\n",
        "                number += 1\n",
        "                task_ID = chr(ord(task_ID) + 1)\n",
        "                i-=1\n",
        "            i+=1\n",
        "        task_StartEnd.loc[number , \"end\"] = i-1\n",
        "\n",
        "        \"設定window size、前後筆資料重複率\"\n",
        "        window_size = 150       #sample rate = 50Hz，取一秒的資料\n",
        "        Repeat_ratio = int(window_size * ((100-50)/100)) \n",
        "        \"切割訓練資料\"\n",
        "        task_ID = 'T'\n",
        "        for i in range(0,6):\n",
        "            start = task_StartEnd.at[i+1 , 'start']\n",
        "            end = task_StartEnd.at[i+1 , 'end'] + 1\n",
        "\n",
        "            start = start + 50      #刪除開始後1秒的步態資料\n",
        "            end = end - 50          #刪除結束前1秒的步態資料\n",
        "\n",
        "            total = end - start\n",
        "            training_set_src = data_src.iloc[start:end,5:colume].values #取7顆IMU的9軸資料，並且依照動作T、U、V、W、X、Y動作分段取\n",
        "            training_set_trg1 = data_trg1.iloc[start:end,5:colume].values #取7顆IMU的9軸資料，並且依照動作T、U、V、W、X、Y動作分段取\n",
        "            training_set_trg2 = data_trg2.iloc[start:end,5:colume].values #取7顆IMU的9軸資料，並且依照動作T、U、V、W、X、Y動作分段取\n",
        "            training_set_trg3 = data_trg3.iloc[start:end,5:colume].values #取7顆IMU的9軸資料，並且依照動作T、U、V、W、X、Y動作分段取\n",
        "            training_set_trg4 = data_trg4.iloc[start:end,5:colume].values #取7顆IMU的9軸資料，並且依照動作T、U、V、W、X、Y動作分段取\n",
        "            training_set_trg5 = data_trg5.iloc[start:end,5:colume].values #取7顆IMU的9軸資料，並且依照動作T、U、V、W、X、Y動作分段取\n",
        "            training_set_trg6 = data_trg6.iloc[start:end,5:colume].values #取7顆IMU的9軸資料，並且依照動作T、U、V、W、X、Y動作分段取\n",
        "            \"Z-score Standardization\"\n",
        "            training_set_src = stats.zscore(training_set_src)\n",
        "            training_set_trg1 = stats.zscore(training_set_trg1)\n",
        "            training_set_trg2 = stats.zscore(training_set_trg2)\n",
        "            training_set_trg3 = stats.zscore(training_set_trg3)\n",
        "            training_set_trg4 = stats.zscore(training_set_trg4)\n",
        "            training_set_trg5 = stats.zscore(training_set_trg5)\n",
        "            training_set_trg6 = stats.zscore(training_set_trg6)\n",
        "\n",
        "            for j in range(0 , total , Repeat_ratio):\n",
        "                if j + window_size <= total:\n",
        "                    gait_x_src.append(training_set_src[j:j + window_size])\n",
        "                    gait_y_src.append(scale_score)                   #切好的資料lable\n",
        "                    gait_x_trg1.append(training_set_trg1[j:j + window_size])\n",
        "                    gait_y_trg1.append(scale_score)                   #切好的資料lable\n",
        "                    gait_x_trg2.append(training_set_trg2[j:j + window_size])\n",
        "                    gait_y_trg2.append(scale_score)                   #切好的資料lable\n",
        "                    gait_x_trg3.append(training_set_trg3[j:j + window_size])\n",
        "                    gait_y_trg3.append(scale_score)                   #切好的資料lable\n",
        "                    gait_x_trg4.append(training_set_trg4[j:j + window_size])\n",
        "                    gait_y_trg4.append(scale_score)                   #切好的資料lable\n",
        "                    gait_x_trg5.append(training_set_trg5[j:j + window_size])\n",
        "                    gait_y_trg5.append(scale_score)                   #切好的資料lable\n",
        "                    gait_x_trg6.append(training_set_trg6[j:j + window_size])\n",
        "                    gait_y_trg6.append(scale_score)                   #切好的資料lable\n",
        "        gait_x_src = np.array(gait_x_src)\n",
        "        gait_y_src = np.array(gait_y_src)\n",
        "        gait_x_trg1 = np.array(gait_x_trg1)\n",
        "        gait_y_trg1 = np.array(gait_y_trg1)\n",
        "        gait_x_trg2 = np.array(gait_x_trg2)\n",
        "        gait_y_trg2 = np.array(gait_y_trg2)\n",
        "        gait_x_trg3 = np.array(gait_x_trg3)\n",
        "        gait_y_trg3 = np.array(gait_y_trg3)\n",
        "        gait_x_trg4 = np.array(gait_x_trg4)\n",
        "        gait_y_trg4 = np.array(gait_y_trg4)\n",
        "        gait_x_trg5 = np.array(gait_x_trg5)\n",
        "        gait_y_trg5 = np.array(gait_y_trg5)\n",
        "        gait_x_trg6 = np.array(gait_x_trg6)\n",
        "        gait_y_trg6 = np.array(gait_y_trg6)\n",
        "        subject_gait_data_x_src.append(gait_x_src)\n",
        "        subject_gait_data_y_src.append(scale_score)\n",
        "        subject_gait_data_x_trg1.append(gait_x_trg1)\n",
        "        subject_gait_data_x_trg2.append(gait_x_trg2)\n",
        "        subject_gait_data_x_trg3.append(gait_x_trg3)\n",
        "        subject_gait_data_x_trg4.append(gait_x_trg4)\n",
        "        subject_gait_data_x_trg5.append(gait_x_trg5)\n",
        "        subject_gait_data_x_trg6.append(gait_x_trg6)\n",
        "        subject_gait_data_y_trg1.append(scale_score)\n",
        "        subject_gait_data_y_trg2.append(scale_score)\n",
        "        subject_gait_data_y_trg3.append(scale_score)\n",
        "        subject_gait_data_y_trg4.append(scale_score)\n",
        "        subject_gait_data_y_trg5.append(scale_score)\n",
        "        subject_gait_data_y_trg6.append(scale_score)\n",
        "\n",
        "\n",
        "    \"打亂受測者的順序\"\n",
        "    r = list(zip(subject_gait_data_x_src,subject_gait_data_y_src,subject_gait_data_x_trg1,subject_gait_data_y_trg1,subject_gait_data_x_trg2,subject_gait_data_y_trg2,\n",
        "                subject_gait_data_x_trg3,subject_gait_data_y_trg3,subject_gait_data_x_trg4,subject_gait_data_y_trg4,subject_gait_data_x_trg5,subject_gait_data_y_trg5,\n",
        "                subject_gait_data_x_trg6,subject_gait_data_y_trg6))\n",
        "    np.random.shuffle(r)\n",
        "    (subject_gait_data_x_src,subject_gait_data_y_src,subject_gait_data_x_trg1,subject_gait_data_y_trg1,subject_gait_data_x_trg2,\n",
        "    subject_gait_data_y_trg2,subject_gait_data_x_trg3,subject_gait_data_y_trg3,subject_gait_data_x_trg4,subject_gait_data_y_trg4,\n",
        "    subject_gait_data_x_trg5,subject_gait_data_y_trg5,subject_gait_data_x_trg6,subject_gait_data_y_trg6) = zip(*r)\n",
        "\n",
        "    \"量表分數 normalization\"\n",
        "    subject_gait_data_y_src = np.array(subject_gait_data_y_src)     #從別的資料型態轉成array\n",
        "    BBS = [[0],[56]]                                        #BBS的分數範圍\n",
        "    BBS_array = np.array(BBS)\n",
        "    scaler = MinMaxScaler(feature_range=(0,1)).fit(BBS_array)\n",
        "    subject_gait_data_y_src = scaler.transform(subject_gait_data_y_src)\n",
        "\n",
        "    subject_gait_data_y_trg1 = np.array(subject_gait_data_y_trg1)     #從別的資料型態轉成array\n",
        "    BBS = [[0],[56]]                                        #BBS的分數範圍\n",
        "    BBS_array = np.array(BBS)\n",
        "    scaler = MinMaxScaler(feature_range=(0,1)).fit(BBS_array)\n",
        "    subject_gait_data_y_trg1 = scaler.transform(subject_gait_data_y_trg1)\n",
        "\n",
        "    subject_gait_data_y_trg2 = np.array(subject_gait_data_y_trg2)     #從別的資料型態轉成array\n",
        "    BBS = [[0],[56]]                                        #BBS的分數範圍\n",
        "    BBS_array = np.array(BBS)\n",
        "    scaler = MinMaxScaler(feature_range=(0,1)).fit(BBS_array)\n",
        "    subject_gait_data_y_trg2 = scaler.transform(subject_gait_data_y_trg2)\n",
        "\n",
        "    subject_gait_data_y_trg3 = np.array(subject_gait_data_y_trg3)     #從別的資料型態轉成array\n",
        "    BBS = [[0],[56]]                                        #BBS的分數範圍\n",
        "    BBS_array = np.array(BBS)\n",
        "    scaler = MinMaxScaler(feature_range=(0,1)).fit(BBS_array)\n",
        "    subject_gait_data_y_trg3 = scaler.transform(subject_gait_data_y_trg3)\n",
        "\n",
        "    subject_gait_data_y_trg4 = np.array(subject_gait_data_y_trg4)     #從別的資料型態轉成array\n",
        "    BBS = [[0],[56]]                                        #BBS的分數範圍\n",
        "    BBS_array = np.array(BBS)\n",
        "    scaler = MinMaxScaler(feature_range=(0,1)).fit(BBS_array)\n",
        "    subject_gait_data_y_trg4 = scaler.transform(subject_gait_data_y_trg4)\n",
        "\n",
        "    subject_gait_data_y_trg5 = np.array(subject_gait_data_y_trg5)     #從別的資料型態轉成array\n",
        "    BBS = [[0],[56]]                                        #BBS的分數範圍\n",
        "    BBS_array = np.array(BBS)\n",
        "    scaler = MinMaxScaler(feature_range=(0,1)).fit(BBS_array)\n",
        "    subject_gait_data_y_trg5 = scaler.transform(subject_gait_data_y_trg5)\n",
        "\n",
        "    subject_gait_data_y_trg6 = np.array(subject_gait_data_y_trg6)     #從別的資料型態轉成array\n",
        "    BBS = [[0],[56]]                                        #BBS的分數範圍\n",
        "    BBS_array = np.array(BBS)\n",
        "    scaler = MinMaxScaler(feature_range=(0,1)).fit(BBS_array)\n",
        "    subject_gait_data_y_trg6 = scaler.transform(subject_gait_data_y_trg6)\n",
        "\n",
        "    savepath = 'save/'\n",
        "    if not os.path.isdir(savepath):\n",
        "        os.mkdir(savepath)\n",
        "    savepath = savepath + 'S2T(1-1)/'\n",
        "    if not os.path.isdir(savepath):\n",
        "        os.mkdir(savepath)\n",
        "\n",
        "    n=0\n",
        "    epoch_range = [500]                             #設定epoch實驗參數\n",
        "    batch_size_range = [64]\n",
        "\n",
        "\n",
        "\n",
        "    for epochs in epoch_range:                    #實驗epoch\n",
        "        for batch_size in batch_size_range:       #實驗batch size\n",
        "            n=3\n",
        "            localtime = time.localtime(time.time())\n",
        "            savepath = 'save/S2T(1-1)/' +str(localtime[0])+str(localtime[1]).zfill(2)+str(localtime[2]).zfill(2)+'v'+str(n)+'_sensor'+str(sensor)+'/'\n",
        "            if not os.path.isdir(savepath):\n",
        "                os.mkdir(savepath)\n",
        "            val_times = 5       # k fold cross validation\n",
        "            MAE_average1 , MAE_average2 , MAE_average3 , MAE_average4 , MAE_average5 , MAE_average6 = 0,0,0,0,0,0\n",
        "            times = 0\n",
        "            subject_size = len(subject_gait_data_x_src)\n",
        "\n",
        "            for val in range(val_times):\n",
        "                times+=1\n",
        "\n",
        "                x_train_src, y_train_src, x_test_src, y_test_src = split_and_shuffle(subject_gait_data_x_src, subject_gait_data_y_src,val_times,subject_size,val)\n",
        "\n",
        "                x_train1, y_train1, x_test1, y_test1 = split_and_shuffle(subject_gait_data_x_trg1, subject_gait_data_y_trg1,val_times,subject_size,val)\n",
        "\n",
        "                x_train2, y_train2, x_test2, y_test2 = split_and_shuffle(subject_gait_data_x_trg2, subject_gait_data_y_trg2,val_times,subject_size,val)\n",
        "                \n",
        "                x_train3, y_train3, x_test3, y_test3 = split_and_shuffle(subject_gait_data_x_trg3, subject_gait_data_y_trg3,val_times,subject_size,val)\n",
        "\n",
        "                x_train4, y_train4, x_test4, y_test4 = split_and_shuffle(subject_gait_data_x_trg4, subject_gait_data_y_trg4,val_times,subject_size,val)\n",
        "\n",
        "                x_train5, y_train5, x_test5, y_test5 = split_and_shuffle(subject_gait_data_x_trg5, subject_gait_data_y_trg5,val_times,subject_size,val)\n",
        "\n",
        "                x_train6, y_train6, x_test6, y_test6 = split_and_shuffle(subject_gait_data_x_trg6, subject_gait_data_y_trg6,val_times,subject_size,val)\n",
        "\n",
        "\n",
        "\n",
        "                src_train_dataset = MyDataset(x_train_src, y_train_src)\n",
        "                src_test_dataset = MyDataset(x_test_src, y_test_src)\n",
        "                trg_train_dataset1 = MyDataset(x_train1, y_train1)\n",
        "                trg_test_dataset1 = MyDataset(x_test1, y_test1)\n",
        "                trg_train_dataset2 = MyDataset(x_train2, y_train2)\n",
        "                trg_test_dataset2 = MyDataset(x_test2, y_test2)\n",
        "                trg_train_dataset3 = MyDataset(x_train3, y_train3)\n",
        "                trg_test_dataset3 = MyDataset(x_test3, y_test3)\n",
        "                trg_train_dataset4 = MyDataset(x_train4, y_train4)\n",
        "                trg_test_dataset4 = MyDataset(x_test4, y_test4)\n",
        "                trg_train_dataset5 = MyDataset(x_train5, y_train5)\n",
        "                trg_test_dataset5 = MyDataset(x_test5, y_test5)\n",
        "                trg_train_dataset6 = MyDataset(x_train6, y_train6)\n",
        "                trg_test_dataset6 = MyDataset(x_test6, y_test6)\n",
        "\n",
        "                src_train_loader = DataLoader(src_train_dataset, batch_size=64, shuffle=False)\n",
        "                src_test_loader = DataLoader(src_test_dataset, batch_size=64, shuffle=False)\n",
        "                trg_train_loader1 = DataLoader(trg_train_dataset1, batch_size=64, shuffle=False)\n",
        "                trg_test_loader1 = DataLoader(trg_test_dataset1, batch_size=64, shuffle=False)\n",
        "                trg_train_loader2 = DataLoader(trg_train_dataset2, batch_size=64, shuffle=False)\n",
        "                trg_test_loader2 = DataLoader(trg_test_dataset2, batch_size=64, shuffle=False)\n",
        "                trg_train_loader3 = DataLoader(trg_train_dataset3, batch_size=64, shuffle=False)\n",
        "                trg_test_loader3 = DataLoader(trg_test_dataset3, batch_size=64, shuffle=False)\n",
        "                trg_train_loader4 = DataLoader(trg_train_dataset4, batch_size=64, shuffle=False)\n",
        "                trg_test_loader4 = DataLoader(trg_test_dataset4, batch_size=64, shuffle=False)\n",
        "                trg_train_loader5 = DataLoader(trg_train_dataset5, batch_size=64, shuffle=False)\n",
        "                trg_test_loader5 = DataLoader(trg_test_dataset5, batch_size=64, shuffle=False)\n",
        "                trg_train_loader6 = DataLoader(trg_train_dataset6, batch_size=64, shuffle=False)\n",
        "                trg_test_loader6 = DataLoader(trg_test_dataset6, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "                # 訓練部分\n",
        "                F = FeatureExtractor().to(DEVICE)\n",
        "                C = Classifier().to(DEVICE)\n",
        "                optimizer = optim.Adam(list(F.parameters()) + list(C.parameters()),lr=0.0005)\n",
        "                criterion = nn.MSELoss()\n",
        "                criterion_mae = nn.L1Loss()\n",
        "\n",
        "                for epoch in range(epochs):\n",
        "                    F.train()\n",
        "                    C.train()\n",
        "                    for i,data in enumerate(src_train_loader):\n",
        "                        x_batch, y_batch = data\n",
        "                        input=x_batch.to(DEVICE)\n",
        "                        input = input.permute(0,2,1)\n",
        "                        label=y_batch.to(DEVICE)\n",
        "                        optimizer.zero_grad()\n",
        "                        feature = F(input)\n",
        "                        outputs = C(feature)\n",
        "                        loss = criterion(outputs, label)\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # 保存模型\n",
        "                feature_extractor_filename = f'FeatureExtractor_fold{val}.pth'\n",
        "                classifier_filename = f'Classifier_fold{val}.pth'\n",
        "                torch.save(F.state_dict(), os.path.join(savepath, feature_extractor_filename))\n",
        "                torch.save(C.state_dict(), os.path.join(savepath, classifier_filename))\n",
        "\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    F.eval()\n",
        "                    C.eval()\n",
        "                    test_loss1 ,test_loss2 ,test_loss3 ,test_loss4 ,test_loss5 ,test_loss6 = 0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 \n",
        "                    for i,data in enumerate(trg_test_loader1):\n",
        "                        x_batch, y_batch = data\n",
        "                        input=x_batch.to(DEVICE)\n",
        "                        label=y_batch.to(DEVICE)\n",
        "                        input = input.permute(0,2,1)\n",
        "                        feature = F(input)\n",
        "                        outputs = C(feature)\n",
        "                        \"將量表分數轉換成原來分數\"\n",
        "                        y_test_tran = label.cpu().detach().numpy()\n",
        "                        y_pred_tran = outputs.cpu().detach().numpy()\n",
        "\n",
        "                        y_test_tran = scaler.inverse_transform(y_test_tran)\n",
        "                        y_pred_tran = scaler.inverse_transform(y_pred_tran)\n",
        "                        y_pred_tran =np.around(y_pred_tran)\n",
        "\n",
        "                        y_test_tran_tensor = torch.tensor(y_test_tran, dtype=torch.float32).to(DEVICE)\n",
        "                        y_pred_tran_tensor = torch.tensor(y_pred_tran, dtype=torch.float32).to(DEVICE)\n",
        "                        loss = criterion_mae(y_pred_tran_tensor, y_test_tran_tensor)\n",
        "                        test_loss1 += loss.item()\n",
        "                    test_loss1 /= len(trg_test_loader1)\n",
        "                    print(f'Test Loss1: {test_loss1:.4f}')\n",
        "                    MAE_average1 = MAE_average1 + test_loss1\n",
        "\n",
        "                    for i,data in enumerate(trg_test_loader2):\n",
        "                        x_batch, y_batch = data\n",
        "                        input=x_batch.to(DEVICE)\n",
        "                        label=y_batch.to(DEVICE)\n",
        "                        input = input.permute(0,2,1)\n",
        "                        feature = F(input)\n",
        "                        outputs = C(feature)\n",
        "                        \"將量表分數轉換成原來分數\"\n",
        "                        y_test_tran = label.cpu().detach().numpy()\n",
        "                        y_pred_tran = outputs.cpu().detach().numpy()\n",
        "\n",
        "                        y_test_tran = scaler.inverse_transform(y_test_tran)\n",
        "                        y_pred_tran = scaler.inverse_transform(y_pred_tran)\n",
        "                        y_pred_tran =np.around(y_pred_tran)\n",
        "\n",
        "                        y_test_tran_tensor = torch.tensor(y_test_tran, dtype=torch.float32).to(DEVICE)\n",
        "                        y_pred_tran_tensor = torch.tensor(y_pred_tran, dtype=torch.float32).to(DEVICE)\n",
        "                        loss = criterion_mae(y_pred_tran_tensor, y_test_tran_tensor)\n",
        "                        test_loss2 += loss.item()\n",
        "                    test_loss2 /= len(trg_test_loader2)\n",
        "                    print(f'Test Loss2: {test_loss2:.4f}')\n",
        "                    MAE_average2 = MAE_average2 + test_loss2\n",
        "\n",
        "                    for i,data in enumerate(trg_test_loader3):\n",
        "                        x_batch, y_batch = data\n",
        "                        input=x_batch.to(DEVICE)\n",
        "                        label=y_batch.to(DEVICE)\n",
        "                        input = input.permute(0,2,1)\n",
        "                        feature = F(input)\n",
        "                        outputs = C(feature)\n",
        "                        \"將量表分數轉換成原來分數\"\n",
        "                        y_test_tran = label.cpu().detach().numpy()\n",
        "                        y_pred_tran = outputs.cpu().detach().numpy()\n",
        "\n",
        "                        y_test_tran = scaler.inverse_transform(y_test_tran)\n",
        "                        y_pred_tran = scaler.inverse_transform(y_pred_tran)\n",
        "                        y_pred_tran =np.around(y_pred_tran)\n",
        "\n",
        "                        y_test_tran_tensor = torch.tensor(y_test_tran, dtype=torch.float32).to(DEVICE)\n",
        "                        y_pred_tran_tensor = torch.tensor(y_pred_tran, dtype=torch.float32).to(DEVICE)\n",
        "                        loss = criterion_mae(y_pred_tran_tensor, y_test_tran_tensor)\n",
        "                        test_loss3 += loss.item()\n",
        "                    test_loss3 /= len(trg_test_loader3)\n",
        "                    print(f'Test Loss3: {test_loss3:.4f}')\n",
        "                    MAE_average3 = MAE_average3 + test_loss3\n",
        "\n",
        "                    for i,data in enumerate(trg_test_loader4):\n",
        "                        x_batch, y_batch = data\n",
        "                        input=x_batch.to(DEVICE)\n",
        "                        label=y_batch.to(DEVICE)\n",
        "                        input = input.permute(0,2,1)\n",
        "                        feature = F(input)\n",
        "                        outputs = C(feature)\n",
        "                        \"將量表分數轉換成原來分數\"\n",
        "                        y_test_tran = label.cpu().detach().numpy()\n",
        "                        y_pred_tran = outputs.cpu().detach().numpy()\n",
        "\n",
        "                        y_test_tran = scaler.inverse_transform(y_test_tran)\n",
        "                        y_pred_tran = scaler.inverse_transform(y_pred_tran)\n",
        "                        y_pred_tran =np.around(y_pred_tran)\n",
        "\n",
        "                        y_test_tran_tensor = torch.tensor(y_test_tran, dtype=torch.float32).to(DEVICE)\n",
        "                        y_pred_tran_tensor = torch.tensor(y_pred_tran, dtype=torch.float32).to(DEVICE)\n",
        "                        loss = criterion_mae(y_pred_tran_tensor, y_test_tran_tensor)\n",
        "                        test_loss4 += loss.item()\n",
        "                    test_loss4 /= len(trg_test_loader4)\n",
        "                    print(f'Test Loss4: {test_loss4:.4f}')\n",
        "                    MAE_average4 = MAE_average4 + test_loss4\n",
        "\n",
        "                    for i,data in enumerate(trg_test_loader5):\n",
        "                        x_batch, y_batch = data\n",
        "                        input=x_batch.to(DEVICE)\n",
        "                        label=y_batch.to(DEVICE)\n",
        "                        input = input.permute(0,2,1)\n",
        "                        feature = F(input)\n",
        "                        outputs = C(feature)\n",
        "                        \"將量表分數轉換成原來分數\"\n",
        "                        y_test_tran = label.cpu().detach().numpy()\n",
        "                        y_pred_tran = outputs.cpu().detach().numpy()\n",
        "\n",
        "                        y_test_tran = scaler.inverse_transform(y_test_tran)\n",
        "                        y_pred_tran = scaler.inverse_transform(y_pred_tran)\n",
        "                        y_pred_tran =np.around(y_pred_tran)\n",
        "\n",
        "                        y_test_tran_tensor = torch.tensor(y_test_tran, dtype=torch.float32).to(DEVICE)\n",
        "                        y_pred_tran_tensor = torch.tensor(y_pred_tran, dtype=torch.float32).to(DEVICE)\n",
        "                        loss = criterion_mae(y_pred_tran_tensor, y_test_tran_tensor)\n",
        "                        test_loss5 += loss.item()\n",
        "                    test_loss5 /= len(trg_test_loader5)\n",
        "                    print(f'Test Loss5: {test_loss5:.4f}')\n",
        "                    MAE_average5 = MAE_average5 + test_loss5\n",
        "\n",
        "                    for i,data in enumerate(trg_test_loader6):\n",
        "                        x_batch, y_batch = data\n",
        "                        input=x_batch.to(DEVICE)\n",
        "                        label=y_batch.to(DEVICE)\n",
        "                        input = input.permute(0,2,1)\n",
        "                        feature = F(input)\n",
        "                        outputs = C(feature)\n",
        "                        \"將量表分數轉換成原來分數\"\n",
        "                        y_test_tran = label.cpu().detach().numpy()\n",
        "                        y_pred_tran = outputs.cpu().detach().numpy()\n",
        "\n",
        "                        y_test_tran = scaler.inverse_transform(y_test_tran)\n",
        "                        y_pred_tran = scaler.inverse_transform(y_pred_tran)\n",
        "                        y_pred_tran =np.around(y_pred_tran)\n",
        "\n",
        "                        y_test_tran_tensor = torch.tensor(y_test_tran, dtype=torch.float32).to(DEVICE)\n",
        "                        y_pred_tran_tensor = torch.tensor(y_pred_tran, dtype=torch.float32).to(DEVICE)\n",
        "                        loss = criterion_mae(y_pred_tran_tensor, y_test_tran_tensor)\n",
        "                        test_loss6 += loss.item()\n",
        "                    test_loss6 /= len(trg_test_loader6)\n",
        "                    print(f'Test Loss6: {test_loss6:.4f}')\n",
        "                    MAE_average6 = MAE_average6 + test_loss6\n",
        "\n",
        "            print(\"BBS MAE:\")\n",
        "            print(MAE_average1/val_times)\n",
        "            print(MAE_average2/val_times)\n",
        "            print(MAE_average3/val_times)\n",
        "            print(MAE_average4/val_times)\n",
        "            print(MAE_average5/val_times)\n",
        "            print(MAE_average6/val_times)   \n",
        "            f = open(savepath + '實驗結果.txt','a')\n",
        "            f.write('window size = '+ str(window_size)+'\\n')\n",
        "            f.write('epoch = ' + str(epochs) + '\\n')\n",
        "            f.write('batch size = ' + str(batch_size) + '\\n')\n",
        "            f.write('MAE1 = ' + str(MAE_average1/val_times) + '\\n')\n",
        "            f.write('MAE2 = ' + str(MAE_average2/val_times) + '\\n')\n",
        "            f.write('MAE3 = ' + str(MAE_average3/val_times) + '\\n')\n",
        "            f.write('MAE4 = ' + str(MAE_average4/val_times) + '\\n')\n",
        "            f.write('MAE5 = ' + str(MAE_average5/val_times) + '\\n')\n",
        "            f.write('MAE6 = ' + str(MAE_average6/val_times) + '\\n')\n",
        "            f.close()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
